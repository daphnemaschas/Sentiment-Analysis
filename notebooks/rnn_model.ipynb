{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bed62",
   "metadata": {},
   "source": [
    "# Réseau Neuronal Récurrent (RNN Simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ea338",
   "metadata": {},
   "source": [
    "## Création d'un vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71ede49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccd16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d4bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"airline_sentiment\", \"text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba50936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('@'))\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('#'))\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(remove_mentions).apply(remove_hashtags).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf3630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               what said.\n",
       "1        plus you've added commercials to the experienc...\n",
       "2        i didn't today... must mean i need to take ano...\n",
       "3        it's really aggressive to blast obnoxious \"ent...\n",
       "4                 and it's a really big bad thing about it\n",
       "                               ...                        \n",
       "14635    thank you we got on a different flight to chic...\n",
       "14636    leaving over 20 minutes late flight. no warnin...\n",
       "14637                    please bring american airlines to\n",
       "14638    you have my money, you change my flight, and d...\n",
       "14639    we have 8 ppl so we need 2 know how many seats...\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298641cd",
   "metadata": {},
   "source": [
    "On veut ensuite retirer les mots vides (Stop Words) (eg: \"the\", \"a\", ...) qui n'apportent pas de signification aux sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# Stop words (tu peux ajouter ou enlever des mots selon ton dataset)\n",
    "stop_words = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
    "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",\n",
    "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\",\n",
    "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\",\n",
    "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n",
    "    \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "    \"now\", \"dont\", \"wont\", \"im\", \"ive\", \"u\", \"ur\", \"2\", \"4\", \"rt\", \"youve\"\n",
    "])\n",
    "\n",
    "domain_stop_words = set([\"flight\", \"minutes\", \"people\", \"seats\", \"time\", \"thank\", \"pls\", \"please\"])\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    # Mettre en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    # Supprimer mentions et hashtags (optionnel)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Supprimer les stop words\n",
    "    cleaned = ' '.join(word for word in text.split() if word not in stop_words|domain_stop_words)\n",
    "    return cleaned\n",
    "\n",
    "# Appliquer sur le dataframe\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b21a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     said\n",
       "1                  plus added commercials experience tacky\n",
       "2             didnt today must mean need take another trip\n",
       "3        really aggressive blast obnoxious entertainmen...\n",
       "4                                     really big bad thing\n",
       "                               ...                        \n",
       "14635                                got different chicago\n",
       "14636    leaving 20 late warnings communication 15 late...\n",
       "14637                              bring american airlines\n",
       "14638    money change answer phones suggestions make co...\n",
       "14639    8 ppl need know many next plz put us standby next\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
