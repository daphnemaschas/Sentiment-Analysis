{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bed62",
   "metadata": {},
   "source": [
    "# Réseau Neuronal Récurrent (RNN Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8953b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ea338",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccd16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d4bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"airline_sentiment\", \"text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b8711",
   "metadata": {},
   "source": [
    "### Enlever les mentions, hashtags et stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba50936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('@'))\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('#'))\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(remove_mentions).apply(remove_hashtags).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf3630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               what said.\n",
       "1        plus you've added commercials to the experienc...\n",
       "2        i didn't today... must mean i need to take ano...\n",
       "3        it's really aggressive to blast obnoxious \"ent...\n",
       "4                 and it's a really big bad thing about it\n",
       "                               ...                        \n",
       "14635    thank you we got on a different flight to chic...\n",
       "14636    leaving over 20 minutes late flight. no warnin...\n",
       "14637                    please bring american airlines to\n",
       "14638    you have my money, you change my flight, and d...\n",
       "14639    we have 8 ppl so we need 2 know how many seats...\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298641cd",
   "metadata": {},
   "source": [
    "On veut ensuite retirer les mots vides (Stop Words) (eg: \"the\", \"a\", ...) qui n'apportent pas de signification aux sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4372d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\clemm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27035353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Supprimer les stop words\n",
    "    cleaned = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b21a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4412df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     said\n",
       "1                  plus added commercials experience tacky\n",
       "2             didnt today must mean need take another trip\n",
       "3        really aggressive blast obnoxious entertainmen...\n",
       "4                                     really big bad thing\n",
       "                               ...                        \n",
       "14635                                got different chicago\n",
       "14636    leaving 20 late warnings communication 15 late...\n",
       "14637                              bring american airlines\n",
       "14638    money change answer phones suggestions make co...\n",
       "14639    8 ppl need know many next plz put us standby next\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b30dd",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca383d3",
   "metadata": {},
   "source": [
    "On va appliquer le Stemming pour réduire les mots à leur racine (ex. : \"courir\", \"court\", \"courait\" deviennent \"courir\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b496f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\clemm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda49b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d9e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd09909",
   "metadata": {},
   "source": [
    "On aurait aussi pu faire de la lemmatisation qui est plus sophistiquée et qui prend en compte une partie du discours (verbe, nom, etc.) et le mot canonique:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2b4fb",
   "metadata": {},
   "source": [
    "| Méthode       | Avantages                             | Inconvénients                        |\n",
    "| ------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Stemming      | Rapide, simple                        | Peut produire des mots non existants |\n",
    "| Lemmatisation | Plus précis, linguistiquement correct | Un peu plus lent, nécessite spaCy    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201eef3",
   "metadata": {},
   "source": [
    "Mais pour cette première itération on va continuer avec le stemming de nltk par simplicité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f027d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     said\n",
       "1                             plu ad commerci experi tacki\n",
       "2               didnt today must mean need take anoth trip\n",
       "3        realli aggress blast obnoxi entertain guest fa...\n",
       "4                                     realli big bad thing\n",
       "                               ...                        \n",
       "14635                                   got differ chicago\n",
       "14636    leav 20 late warn commun 15 late that call shi...\n",
       "14637                                bring american airlin\n",
       "14638         money chang answer phone suggest make commit\n",
       "14639    8 ppl need know mani next plz put us standbi next\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c952335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ef1dc5f",
   "metadata": {},
   "source": [
    "## Création du Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90b2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
