{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bed62",
   "metadata": {},
   "source": [
    "# Homemade LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8953b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ea338",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccd16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d4bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"airline_sentiment\", \"text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b8711",
   "metadata": {},
   "source": [
    "### Enlever les mentions, hashtags et stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba50936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('@'))\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('#'))\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(remove_mentions).apply(remove_hashtags).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf3630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               what said.\n",
       "1        plus you've added commercials to the experienc...\n",
       "2        i didn't today... must mean i need to take ano...\n",
       "3        it's really aggressive to blast obnoxious \"ent...\n",
       "4                 and it's a really big bad thing about it\n",
       "                               ...                        \n",
       "14635    thank you we got on a different flight to chic...\n",
       "14636    leaving over 20 minutes late flight. no warnin...\n",
       "14637                    please bring american airlines to\n",
       "14638    you have my money, you change my flight, and d...\n",
       "14639    we have 8 ppl so we need 2 know how many seats...\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298641cd",
   "metadata": {},
   "source": [
    "On veut ensuite retirer les mots vides (Stop Words) (eg: \"the\", \"a\", ...) qui n'apportent pas de signification aux sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4372d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27035353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Supprimer les stop words\n",
    "    cleaned = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b21a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4412df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     said\n",
       "1            plus youve added commercials experience tacky\n",
       "2             didnt today must mean need take another trip\n",
       "3        really aggressive blast obnoxious entertainmen...\n",
       "4                                     really big bad thing\n",
       "                               ...                        \n",
       "14635                   thank got different flight chicago\n",
       "14636    leaving 20 minutes late flight warnings commun...\n",
       "14637                       please bring american airlines\n",
       "14638    money change flight dont answer phones suggest...\n",
       "14639    8 ppl need 2 know many seats next flight plz p...\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b30dd",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca383d3",
   "metadata": {},
   "source": [
    "On va appliquer le Stemming pour réduire les mots à leur racine (ex. : \"courir\", \"court\", \"courait\" deviennent \"courir\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b496f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda49b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d9e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd09909",
   "metadata": {},
   "source": [
    "On aurait aussi pu faire de la lemmatisation qui est plus sophistiquée et qui prend en compte une partie du discours (verbe, nom, etc.) et le mot canonique:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2b4fb",
   "metadata": {},
   "source": [
    "| Méthode       | Avantages                             | Inconvénients                        |\n",
    "| ------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Stemming      | Rapide, simple                        | Peut produire des mots non existants |\n",
    "| Lemmatisation | Plus précis, linguistiquement correct | Un peu plus lent, nécessite spaCy    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201eef3",
   "metadata": {},
   "source": [
    "Mais pour cette première itération on va continuer avec le stemming de nltk par simplicité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f027d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     said\n",
       "1                        plu youv ad commerci experi tacki\n",
       "2               didnt today must mean need take anoth trip\n",
       "3        realli aggress blast obnoxi entertain guest fa...\n",
       "4                                     realli big bad thing\n",
       "                               ...                        \n",
       "14635                      thank got differ flight chicago\n",
       "14636    leav 20 minut late flight warn commun 15 minut...\n",
       "14637                          pleas bring american airlin\n",
       "14638    money chang flight dont answer phone suggest m...\n",
       "14639    8 ppl need 2 know mani seat next flight plz pu...\n",
       "Name: cleaned_text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7397644",
   "metadata": {},
   "source": [
    "Tokenisation simple car on a déjà nettoyé le texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad85e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0a117",
   "metadata": {},
   "source": [
    "On split en train, test et validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e24032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a47b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[\"cleaned_text\"], df[\"airline_sentiment\"]\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1dc5f",
   "metadata": {},
   "source": [
    "## Création du Vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c711db",
   "metadata": {},
   "source": [
    "On va maintenant utiliser une classe d'outils pour parcourir l'ensemble du texte d'entraînement (X_train).\n",
    "Puis générer une structure qui mappe chaque mot unique à un entier (index). C'est le vocabulaire.\n",
    "\n",
    "On va aussi définir un index spécial pour les mots inconnus (Out-Of-Vocabulary ou OOV).\n",
    "\n",
    "Le vocabulaire ne doit être construit QUE sur le jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a711ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b90b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = [\"<PAD>\", \"<UNK>\"] + [w for w, f in counter.items() if f >= min_freq]\n",
    "    word2idx = {w:i for i, w in enumerate(vocab)}\n",
    "\n",
    "    return vocab, word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78750528",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word2idx = build_vocab(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a9ba1",
   "metadata": {},
   "source": [
    "Transformation de chaque tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97067dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, word2idx):\n",
    "    return [word2idx.get(t, word2idx[\"<UNK>\"]) for t in tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f9bb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def encode_dataset(texts, word2idx):\n",
    "    sequences = [torch.tensor(encode(t, word2idx)) for t in texts]\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=word2idx[\"<PAD>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12edd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = encode_dataset(X_train, word2idx)\n",
    "X_val_tensor   = encode_dataset(X_val, word2idx)\n",
    "X_test_tensor  = encode_dataset(X_test, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da716b8",
   "metadata": {},
   "source": [
    "Ensuite on encode les labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66355d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = torch.tensor(le.fit_transform(y_train))\n",
    "y_val_enc   = torch.tensor(le.transform(y_val))\n",
    "y_test_enc  = torch.tensor(le.transform(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993797bb",
   "metadata": {},
   "source": [
    "Après on crée un dataset et dataloader avec pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "261e4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_enc)\n",
    "val_dataset   = TensorDataset(X_val_tensor, y_val_enc)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_enc)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e080181",
   "metadata": {},
   "source": [
    "Ensuite on initialise le modèle, c'est un RNN (simple LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1fd2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eb2afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm_model_from_scratch import ManualLSTM\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e26381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = ManualLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_classes=len(le.classes_)\n",
    ").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ddcc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8a4c4",
   "metadata": {},
   "source": [
    "Ensuite on entraine en utilisant la loss et l'optimizer définit plus haut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9db2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=3, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Pour la barre de progression tqdm\n",
    "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "        for X_batch, y_batch in train_iter:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Mise à jour de la barre de progression\n",
    "            train_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                preds = model(X_batch).argmax(dim=1)\n",
    "\n",
    "                correct += (preds == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Train Loss = {total_loss:.3f} | Val Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7a2b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss = 298.614 | Val Acc = 0.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss = 246.872 | Val Acc = 0.7185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss = 218.428 | Val Acc = 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss = 194.101 | Val Acc = 0.7360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss = 167.097 | Val Acc = 0.7511\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f852d2",
   "metadata": {},
   "source": [
    "Evaluation finale sur le test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2444d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.84      0.85       918\n",
      "     neutral       0.57      0.59      0.58       310\n",
      "    positive       0.61      0.66      0.63       236\n",
      "\n",
      "    accuracy                           0.76      1464\n",
      "   macro avg       0.68      0.69      0.69      1464\n",
      "weighted avg       0.76      0.76      0.76      1464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        preds = model(X.to(\"cpu\")).argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
